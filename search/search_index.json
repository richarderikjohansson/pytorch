{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation for PyTorch course","text":"<p>Here will all documentation be stored for the YouTube course  relating to PyTorch. The course is  hosted for free on YouTube and can be found here. </p> <p>The course have 10 chapters in total, with exercises tied to each  chapter. GitHub repository, with the exercises can be found here and documentation for the course can be found here </p>"},{"location":"Chapters/00/","title":"PyTorch Fundamentals","text":"<p>This chapter was entirely about working with tensors in  pytorch, as well working with random numbers. See  documentation on <code>torch.Tensor()</code> here, for creating <code>torch.rand()</code> here and seeding with <code>torch.random</code> here </p> <p>It is mostly about learning to navigate the documentation, and to get the habit of  turning to this when needed.</p>"},{"location":"Chapters/01/","title":"PyTorch Workflow Fundamentals","text":"<p>This chapter will cover topics such as:</p> <ul> <li>Data (preparing and loading)</li> <li>Building a simple model</li> <li>Fitting the model to data. i.e. training the model</li> <li>Making predictions and evaluating a model (inference)</li> <li>Saving and loading a model</li> </ul> <p> </p>"},{"location":"Chapters/01/#data","title":"Data","text":"<p>A very fundamental caveat around data in Machine Learning is  to split the data into mainly three different groups:</p> <ol> <li>Training data</li> <li>Validation data</li> <li>Test data</li> </ol> <p>The Training data is used to train the model to find the sought  patterns and parameters. The Validation data is used to validate  the model, and to make adjustments to it if needed. Finally  the Test data is used to see if the model is ready to for use.</p> <p>The typical splits is between 60-80% of your total data should be  used for training. 10-20% for validation and 10-20% for testing.  However, validation sets are not always used since the test set  also works as a validation set.</p>"},{"location":"Chapters/01/#building-models","title":"Building Models","text":"<p>What type of model you are building depend entirely on what the purpose of the model is. However, in pytorch you always build a class that  inherits from <code>torch.nn.Module</code>. This class contains all the necessary  building blocks needed to build any Machine Learning model. See nn.Module for the documentation.</p> <p>Below will is an example of a very simple model that will inherit from  <code>nn.Module</code> and is a linear regression model.</p> <pre><code>class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return \n</code></pre> <p>In line 3 in the snippet above is the initialization class called from  <code>nn.Module</code> which contains all necessary methods to compute. One important  thing method seen in the snippet above is the <code>forward()</code>  method, which  overwrites <code>nn.Module</code>s one. This one is where we define what the model is supposed to do. In this case we have not yet defined this.</p> <p>As a summary we have a few essential modules we will be using when building such a simple model as the one above.</p> <ul> <li><code>torch.nn</code> contains all of the building blocks for a neural network</li> <li><code>torch.nn.Parameter</code> contains what parameters that the model should try to learn. often a PyTorch layer which will be set for us.</li> <li><code>torch.nn.Module</code> is the base class for all neural networks, and if you inherit from it, you should overwrite the <code>forward()</code> method.</li> <li><code>torch.optim</code> contain various optimization algorithms.</li> </ul> <p>A good place to look in order to see what various PyTorch modules do is to  look into the PyTorch cheat sheet </p>"},{"location":"Chapters/01/#model-training","title":"Model Training","text":"<p>In order to make the model to predict data it has to be trained. It is done  with the training data set. Down below we see essential blocks needed for training the model</p>"},{"location":"Chapters/01/#loss-functions-and-optimizers","title":"Loss functions and Optimizers","text":"<ul> <li>Loss Function A function to measure how wrong the models predictions are</li> <li>Optimizer Uses the loss of the model and adjust the models parameters to make the predictions better, by minimizing the loss function.</li> </ul> <p>To see which built in loss functions ans optimizers that exist within PyTorch see Loss Functions  and  Optimizers. What is  essential in all models is that these have to be picked to the developer, and which is the right one to use depend on the model that is being built.</p> <p>Another important parameter that the developer have to choose is the hyperparameter <code>lr</code> which stand for learning rate. This tells the optimizer how much the parameter values should change in order to decrease the loss. </p>"},{"location":"Chapters/01/#training-loop","title":"Training loop","text":"<p>The training loop is where the model is being trained in order to update the parameter values. There is a few essential steps in this loop that need to happen, and these are listed below:</p> <p>Loop through the data and: 1. forward pass (moving the input data through the network) to give a output on the current parameters  2. calculate the loss with the loss function  3. optimizer zero grad, resets the gradients of the optimized tensor. 4. back propagation moves the data through the network backwards (calculate the loss gradients) 5. optimizer step, refining the parameters (gradient descent)</p>"},{"location":"Chapters/02/","title":"PyTorch Neural Network Classification","text":"<p>In this chapter will how classifications models are developed  in PyTorch. Classification is a very powerful tool and can  be used for more or less any type of data. </p> <p>The core of a classification model is to figure out if  some type of data is something, or not. For example, if a image is  of a dog. The model will then be trained to correctly  interpret whether a image dog and when properly trained will the model be able to figure out if an inputted image  is a dog or not. This is just a very simple example. For this chapters resources visit: PyTorch Neural Network Classification.</p> <p>Another very good resource is Made With ML, developed by Goku Mohandas.</p>"},{"location":"Exercises/00/","title":"PyTorch Fundamentals","text":""},{"location":"Exercises/00/#exercise-1","title":"Exercise 1","text":"<p>A big part of deep learning (and learning to code in general) is getting familiar with the documentation of a certain framework you're using. We'll be using the PyTorch documentation a lot throughout the rest of this course. So I'd recommend spending 10-minutes reading the following (it's okay if you don't get  some things for now, the focus is not yet full understanding, it's awareness):</p> <ul> <li>The tensor datatype has a lot of methods and attributes available, will go  back when needing more information </li> <li>Utilizing CUDA threads is a very good way to speed up the software, instead of running all operations on the CPU. On my current machine (MacBook Air M2)  I do not have CUDA cores available, but will dive into this when I have my  desktop PC running again from my office.</li> </ul>"},{"location":"Exercises/00/#exercise-2","title":"Exercise 2","text":"<p>Create a random tensor with shape <code>(7, 7)</code>.</p> <p>Create this random tensor with <code>torch.rand()</code>:</p> <pre><code>def e2():\n    random_tensor = torch.rand(7, 7)\n    print(\"Shape: \", random_tensor.shape)\n</code></pre>"},{"location":"Exercises/00/#exercise-3","title":"Exercise 3","text":"<p>Perform matrix multiplication on the tensor from Exercise 2 with another random tensor with shape <code>(1, 7)</code></p> <p>To perform matrix multiplication we do this with <code>torch.matmul</code>. But we have to transpose the second tensor since the number of rows do not align with the number or columns in the first tensor. This is done with the  <code>transpose()</code> before making the matrix multiplication:</p> <pre><code>def e3():\n    rt1 = torch.rand(7, 7)\n    rt2 = torch.rand(1, 7)\n    rt2_t = rt2.transpose(0, 1)\n    mat_multi = torch.matmul(rt1, rt2_t)\n\n    print(f\"Matrix multiplication {mat_multi=}\",\n          f\"With shape {mat_multi.shape=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-4","title":"Exercise 4","text":"<p>Set the random seed to <code>0</code> and do exercises 2 and 3 again</p> <p>We are setting the random seed on the CPU with <code>torch.random.manual_seed()</code>. Then we perform the same as in  exercises 2 and 3: </p> <pre><code>def e4():\n    torch.random.manual_seed(0)\n    rt1 = torch.rand(size=(7, 7))\n    rt2 = torch.rand(size=(1, 7))\n    rt2_t = rt2.transpose(0, 1)\n    mat_multi = torch.matmul(rt1, rt2_t)\n    print(f\"Matrix multiplication {mat_multi=}\",\n          f\"With shape {mat_multi.shape=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-5","title":"Exercise 5","text":"<p>Speaking of random seeds, we saw how to set it with <code>torch.manual_seed()</code> but is there a GPU equivalent?  (hint: you'll need to look into the documentation for <code>torch.cuda</code> for this one)</p> <p>Since I do not have CUDA cores available we skip this exercise for now. But you set the random seed  on one CUDA core with <code>torch.cuda.manual_seed()</code></p> <pre><code>def e5():\n    print(f\"CUDA: {torch.cuda.is_available()=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-6","title":"Exercise 6","text":"<p>Create two random tensors of shape <code>(2, 3)</code> and send them both to the GPU (you'll need access to a GPU for this).  Set <code>torch.manual_seed(1234)</code> when creating the tensors (this doesn't have to be the GPU random seed).</p> <p>As in exercise 5, we do not have CUDA cores available. But we can check this with <code>torch.cuda.is_available()</code>. So  I do this and sets the seed on the CUDA cores if they are available, if not I seed the CPU and do what is asked and  returning the tensors, since they are used in the next exercise:</p> <pre><code>def e6():\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(1234)\n    else:\n        torch.random.manual_seed(1234)\n\n    rt1 = torch.rand(2, 3)\n    rt2 = torch.rand(2, 3)\n    print(f\"{rt1=}\")\n    print(f\"{rt2=}\")\n\n    return rt1, rt2\n</code></pre>"},{"location":"Exercises/00/#exercise-7","title":"Exercise 7","text":"<p>Perform a matrix multiplication on the tensors you created in exercise 6 (again, you may have to adjust the shapes of one of the tensors).</p> <p>Since the number of rows in the second tensor not is matching the number of columns in the first tensor I have to transpose again before doing the matrix multiplication with <code>torch.matmul</code>. I also return the  matrix product since I have to use this in exercises 8 and 9:</p> <pre><code>def e7(rt1: torch.Tensor, rt2: torch.Tensor):\n    rt2_t = rt2.transpose(0, 1)\n    mat_multi = torch.matmul(rt1, rt2_t)\n    return mat_multi\n    print(f\"{mat_multi=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-8","title":"Exercise 8","text":"<p>Find the maximum and minimum values of the output of exercise 7</p> <p>This is done with the method <code>max()</code> and <code>min()</code> which is attributes to the tensor class:</p> <pre><code>def e8(mat_multi: torch.Tensor):\n    mx = mat_multi.max()\n    mn = mat_multi.min()\n    print(f\"{mat_multi=}\\n max: {mx=} \\n min: {mn=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-9","title":"Exercise 9","text":"<p>Find the maximum and minimum index values of the output of exercise 7.</p> <p>This is done with the <code>argmin()</code> and <code>argmax()</code> methods:</p> <pre><code>def e9(mat_multi: torch.Tensor):\n    argmx = mat_multi.argmax()\n    argmn = mat_multi.argmin()\n    print(f\"{mat_multi=}\\n argmax: {argmx=} \\n argmin: {argmn=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-10","title":"Exercise 10","text":"<p>Make a random tensor with shape <code>(1, 1, 1, 10)</code> and then create a new tensor with all the 1 dimensions removed to be left with a tensor of shape <code>(10)</code>.  Set the seed to 7 when you create it and print out the first tensor and it's shape as well as the second tensor and it's shape.</p> <p>Once again I set the seed with <code>torch.random.manual_seed()</code> and then I am defining the first multi dimensional tensor with the  <code>size</code> argument. I then create a new tensor by selecting the first indices in the first tensor</p> <pre><code>def e10():\n    torch.random.manual_seed(7)\n    rt1 = torch.rand(size=(1, 1, 1, 10))\n    rt2 = rt1[0, 0, 0]\n\n    print(rt1, rt1.shape)\n    print(rt2, rt2.shape)\n</code></pre>"},{"location":"Exercises/01/","title":"PyTorch Workflow Fundamentals","text":"<p>In this chapter some of the most fundamental parts in  building an Machine Learning model is being gone through. All the way from splitting the data into Training and  Testing sets to saving the trained model.</p> <p>The workflow that this model entails is one with a ground thruth, meaning that we actually create the data from a linear regression  formula. But even though we actually know the parameters that our  model will find in the data will this still be a good first model to build.</p>"},{"location":"Exercises/01/#exercise-1","title":"Exercise 1","text":"<p>Create a straight line dataset using the linear regression formula <code>(weight * X + bias)</code>.  Set weight to 0.3 and bias to 0.9 and split the data into 80% training and 20% testing and  plot the data </p> <pre><code>def e1(self):\n    \"\"\"First exercise of Chapter 1\n\n    In this exercise will data be created using the linear\n    regression formula. The data will be split in training and\n    testing data (80/20 split) and plot the training and testing\n    data for visualization\n    \"\"\"\n    self.logger.info(\"--- Running Exercise 1 ---\")\n    bias = 0.9\n    weight = 0.3\n    start = 0\n    end = 1\n    step = 0.005\n\n    X = torch.arange(start, end, step).unsqueeze(dim=1)\n    y = weight * X + bias\n\n    split = int(0.8 * len(X))\n\n    self.X_train = X[:split]\n    self.y_train = y[:split]\n    self.X_test = X[split:]\n    self.y_test = y[split:]\n\n    self.logger.info(\"Plotting training and testing data\\n\")\n    plot_predictions(self.X_train,\n                     self.y_train,\n                     self.X_test,\n                     self.y_test,\n                     figname=self.assetsdir / \"ch1e1.png\")\n</code></pre> <p>I created the data by utilizing the <code>torch.arange()</code> function and then using the linear regression  formula to create the outputs seen in lines 16 and 17. I then split the data in by finding index of  80% of the data and then using slicing to order the data into training, and testing sets. This can be seen in lines 19-24. I then created a plotting function to plot this data, which also could be used  to plot predictions which is when the model is being used on testing data. Seen in the figure below  is only the training and testing data</p> <p> </p> <pre><code>def plot_predictions(train_data,\n                     train_labels,\n                     test_data,\n                     test_labels,\n                     figname,\n                     predictions=None):\n\n    gs = GridSpec(1, 1)\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(gs[0, 0])\n    ax.set_title(\"Traning and testing data\")\n    ax.scatter(train_data, train_labels, c=\"dimgray\", s=4, label=\"Training Data\")\n    ax.scatter(test_data, test_labels, c=\"orange\",  s=4, label=\"Test Data\")\n\n    if predictions is not None:\n        ax.scatter(test_data,\n                   predictions,\n                   c=\"tomato\",\n                   s=4,\n                   label=\"Predictions\")\n\n    ax.legend()\n    fig.savefig(figname, transparent=False)\n    plt.close()\n</code></pre>"},{"location":"Exercises/01/#exercise-2","title":"Exercise 2","text":"<p>Build a PyTorch model by subclassing <code>nn.Module</code>. In the model should parameters  be initialized using the <code>nn.Parameters</code> used, setting it to random values. One parameter for the weight for the bias. a <code>forward()</code> model should also be created to compute the linear  regression. Also make an instance of the model and print the current parameters on the untrained model.</p> <p>The way of doing this is by creating a new class that inherits from the <code>nn.Module</code> class. The <code>nn.Module</code> class contains more or less every building block necessary for any kind of Neural Net. Below is my implementation:</p> <pre><code>class LinearRegressionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.random.manual_seed(42)\n        self.bias = nn.Parameter(torch.randn(1, \n                                             requires_grad=True, \n                                             dtype=torch.float))\n        self.weight = nn.Parameter(torch.randn(1, \n                                               requires_grad=True, \n                                               dtype=torch.float))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.weight * x + self.bias\n</code></pre> <p>As instructed I created parameters for the bias and the weight using the <code>nn.Parameters</code> class, and setting those parameters to random values with <code>torch.randn</code>. Seen in the creation of these, I set the argument <code>requires_grad</code> to True. This is needed if gradient descent is used  to find these values in the training, which is the most standard way of solving a problem of this  kind. </p> <p>I also implemented a <code>forward()</code> method for this model, which always has to be done when inheriting from <code>nn.Module</code>. This method is used to propagate through the neural net and will do the predictions from the input data.</p> <p>To initialize this model I just call this model, and to print out the current parameters is the method <code>state_dict()</code> called on the newly initialized model object, as seen below</p> <pre><code>def e2(self):\n    \"\"\"Second exercise of Chapter 1\n\n    In this we shall construct a linear regression model with\n    randomized parameters. We should also construct a forward\n    method that gives the prediction from the input and\n    parameters. We should the initiate this model and print out\n    the current parameters.\n    \"\"\"\n    self.logger.info(\"--- Running Exercise 2 ---\")\n    self.logger.info(\"Initiating the model\")\n    self.model = LinearRegressionModel()\n    params = self.model.state_dict()\n    self.logger.info(f\"Current parameters:\\n {params}\\n\")\n</code></pre>"},{"location":"Exercises/01/#exercise-3","title":"Exercise 3","text":"<p>Create a loss function and optimizer using nn.L1Loss() and torch.optim.SGD(params, lr) respectively. Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2. Write a training loop to perform the appropriate training steps for 300 epochs. The training loop should test the model on the test dataset every 20 epochs.</p> <p>The loss function and the optimizer is two of the most fundamental things there is when it comes  to machine learning. </p> <ul> <li> <p>The loss function dictates how the loss is measured, meaning how much the predicted data deviates from the expected output. This function will depend on what type of problem you are trying to solve. But in this exercise I was instructed to use the <code>nn.L1Loss()</code> function, which actually measures the mean absolute error.</p> </li> <li> <p>The optimizer is a function that tells the model how the parameters should be updated. These functions also take the very important hyperparameter <code>lr</code>, which is the learning rate. This  hyperparameter is very important because it tells the model how aggressively the model should update  the models parameters. The optimizer we were instructed to use here implements stochastic gradient descent and is found in the <code>torch.optim</code> module.</p> </li> </ul> <p>The training loop typically follows the below pattern, and in the code snippet below is my implementation</p> <ol> <li>forward pass (moving the input data through the network) to give a output on the current parameters </li> <li>calculate the loss with the loss function </li> <li>optimizer zero grad, resets the gradients of the optimized tensor.</li> <li>back propagation moves the data through the network backwards (calculate the loss gradients)</li> <li>optimizer step, refining the parameters (gradient descent)</li> </ol> <pre><code>    def e3(self, epochs):\n        \"\"\"Third exercise of Chapter 1\n\n        In this exercise we are supposed to develop the training\n        loop and also print out how the loss progresses\n\n        Args:\n            epochs (int): Number of times the model should fun through the NN\n        \"\"\"\n        self.logger.info(\"--- Running Exercise 3 ---\")\n        self.logger.info(\"Creating loss and optimizer functions\")\n        self.loss = nn.L1Loss()\n        self.optimizer = torch.optim.SGD(\n            params=self.model.parameters(),\n            lr=0.01\n        )\n\n        self.epochs = epochs\n        self.logger.info(\"Entering training loop\")\n\n        # training loop\n        for epoch in range(self.epochs):\n\n            # set model into training mode\n            self.model.train()\n\n            # forward pass through network\n            y_pred = self.model(self.X_train)\n\n            # calculate loss\n            loss = self.loss(y_pred, self.y_train)\n\n            # set optimizer to zero gradient\n            self.optimizer.zero_grad()\n\n            # back propagate\n            loss.backward()\n\n            # step the optimizer\n            self.optimizer.step()\n\n            # enter testing mode\n            self.model.eval()\n            with torch.inference_mode():\n                test_pred = self.model(self.X_test)\n                test_loss = self.loss(test_pred, self.y_test)\n\n                if epoch % 20 == 0:\n                    print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n\n        self.logger.info(\"Training done\\n\")\n</code></pre> <p>Below is a figure illustrating how the loss evolves during the training loop</p> <p> </p>"},{"location":"Exercises/01/#exercise-4","title":"Exercise 4","text":"<p>Make predictions with the trained model on the test data. Visualize these predictions against the original training and testing data  (note: you may need to make sure the predictions are not on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).</p> <p>This exercise is all about visualizing how well our model predicts the testing data. To get the predictions I first put the model in evaluate mode with the method <code>eval()</code> and do the prediction in inference mode with  <code>torch.inference_mode()</code>. This can be seen in the code snippet below, and the visualization of the training, testing  and predicted data can be seen in the figure below</p> <p><pre><code>def e4(self):\n    self.logger.info(\"--- Running Exercise 4 ---\")\n    self.model.eval()\n    with torch.inference_mode():\n        predictions = self.model(self.X_test)\n\n    self.logger.info(\"Plotting the predictions\\n\")\n    plot_predictions(\n        train_data=self.X_train,\n        train_labels=self.y_train,\n        test_data=self.X_test,\n        test_labels=self.y_test,\n        figname=self.assetsdir / \"ch1e4.png\",\n        predictions=predictions\n    )\n</code></pre> </p>"},{"location":"Exercises/01/#exercise-5","title":"Exercise 5","text":"<p>Save your trained model's state_dict() to file. Create a new instance of your model class you made in 2. and load in the state_dict() you just saved to it. Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.</p> <p>To save the trained model I created a separate function as can be seen below <pre><code>def save_model(model, name):\n    modeldir = Path(\"models\")\n    basedir = Path(__file__)\n    savedir = basedir.parent / modeldir\n\n    if not savedir.exists():\n        savedir.mkdir()\n\n    torch.save(obj=model.state_dict(),\n               f=savedir / name)\n</code></pre></p> <p>Here I pass in the model as one of the argument and a second argument, name which will be under what name the models parameters will be saved. I also create a directory called <code>models/</code> where the model will be saved. I also created a function to load models as seen in the code snippet  below.</p> <p><pre><code>def load_model(model, name):\n    modeldir = Path(\"models\")\n    basedir = Path(__file__)\n    savedir = basedir.parent / modeldir\n    modelfile = savedir / name\n\n    if modelfile.exists():\n        model.load_state_dict(torch.load(modelfile))\n\n    return model\n</code></pre> In order to load a model you have to create an instance of the <code>LinearRegressionModel</code> and  pass it in as one of the arguments and also the name of the model you want to load. In the code snippet below and the figure below depicts the training data, testing data, predictions from the model trained and predictions from the model loaded.</p> <p><pre><code>def e5(self, name: str):\n    \"\"\"Fifth exercise in Chapter 1\n\n    In this exercise we are saving and loading \n    our trained model. We are also supposed to ensure\n    that our loaded model is giving the same predictions\n    as the model we saved\n\n    Args:\n        name: name of the model to be saved and loaded. \n            Should end with .pth\n    \"\"\"\n    self.logger.info(\"--- Running Exercise 5 ---\")\n\n    # save model\n    save_model(model=self.model, name=name)\n    self.logger.info(f\"Saved {self.model} as {name}\")\n\n    # initiate new model\n    model = LinearRegressionModel()\n    self.logger.info(f\"Current parameters of newly loaded model:\\n{model.state_dict()}\")\n    self.logger.info(f\"Loading model {name}\")\n\n    # load model\n    model_loaded = load_model(model, name)\n    self.logger.info(f\"Current parameters of loaded model:\\n{model_loaded.state_dict()}\")\n\n    # initiate figure object\n    fig = plt.figure(figsize=(10, 7))\n    gs = GridSpec(1, 1)\n\n    # initiate matplotlib axis and set title\n    ax = fig.add_subplot(gs[0, 0])\n    ax.set_title(\"Training, testing data with predictions from trained and loaded model\")\n\n    # plot training and testing data\n    ax.scatter(self.X_train, self.y_train, c=\"dimgray\", s=4, label=\"Training Data\")\n    ax.scatter(self.X_test, self.y_test, c=\"orange\", s=4, label=\"Testing Data\")\n\n    # put models in evaluation mode\n    self.model.eval()\n    model_loaded.eval()\n\n    # get predictions\n    with torch.inference_mode():\n        preds = self.model(self.X_test)\n        preds_loaded = model_loaded(self.X_test)\n\n    # plot predictions\n    self.logger.info(\"Plotting\")\n    ax.scatter(self.X_test, preds, c=\"tomato\", s=4, label=\"Predictions\")\n    ax.plot(self.X_test, preds, label=\"Predictions from loaded model\", linestyle=\"-.\")\n    ax.legend()\n\n    # save figure and close figure\n    fig.savefig(self.assetsdir / \"ch1e5.png\")\n    plt.close()\n</code></pre> </p>"},{"location":"Exercises/02/","title":"PyTorch Neural Network Classification","text":"<p>In this chapter will a classification model be built, which will be try to classify non linear data that is downloaded from sci-kit learn. This  dataset belong to the many toy datasets that sci-kit learn has. </p>"},{"location":"Exercises/02/#exercise-1","title":"Exercise 1","text":"<p>Make a binary classification dataset with sklearns make_moons() method:. For consistency, the dataset should have 1000 samples and  a random state of 42 Turn the data into PyTorch tensors. Split data into 80/20 training and testing sets</p> <p>To load in the dataset will be done with the <code>make_moons()</code> which is  composed by two interleaving half circles. This is loaded from sci-kit learns <code>datasets</code> module as seen in the code snippet. I set the random seed to 42 as instructed and also add some noise to the data.</p> <pre><code>self.X, self.y = make_moons(n_samples=samples,\n                            noise=noise,\n                            random_state=42)\n</code></pre> <p>To convert the data from numpy to tensors I use the <code>from_numpy()</code> method in PyTorch and then split the data into training and testing data from another function  in sci-kit learn, called <code>train_test_split()</code> this function splits the data  randomly and can be seen in the snippet below</p> <pre><code>self._X = torch.from_numpy(self.X).type(torch.float)\nself._y = torch.from_numpy(self.y).type(torch.float)\n\n# set training and testing sets\nself.logger.info(\"Divide dataset into 80/20 train test split, randomly\")\n(self.X_train, self.X_test, self.y_train, self.y_test) = train_test_split(\n    self._X,\n    self._y,\n    test_size=0.2,\n    random_state=42\n)\n</code></pre> <p>I then chose to plot the full dataset to visualize the data, which always is good to do.</p> <p> </p>"},{"location":"Exercises/02/#exercise-2","title":"Exercise 2","text":"<p>Build a model by subclassing nn.Module that incorporates non-linear activation functions and is capable of fitting the data from exercise 1.</p> <p>In this exercise I will build a model that will be able to make the classification of the make moons dataset by labeling correctly. One very important insight in this exercise is that this data differs in several way in comparison to the data I created in Chapter 1. Firstly, it is a classification problem, and  not a regression problem. Secondly the data is non-linear, which mean that a model similar to the one developed in Chapter 1 will not be sufficient to classify the data.</p> <p>This mean that we have to include non-linearity in our neural network. PyTorch have  lots of functions that does this, and some work better than others, depending on the  dataset that the model is built on. The one used in this exercise is the  Rectified Linear Unit(ReLU) function with torch.nn.ReLU,  which can be seen in the figure below</p> <p> </p> <p>The model setup is very similar to the one model developed in Chapter 1, but  with a few important changes. I now is setting up in total of three layers, which essentially consist  of:</p> <ul> <li>Input layer with two input features (x, y) that has 10 output features</li> <li>One hidden layer with 10 input features and 10 output features</li> <li>Output layer with 10 input features and 1 output feature (the classification)</li> </ul> <p>As seen in the <code>__init__</code> constructor in the snippet below. Be also define the  non linear activation function, <code>nn.ReLU()</code> here.</p> <pre><code>class MoonsModel_relu(nn.Module):\n    \"\"\"\n    Classifier model with ReLU activation\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize layers and activation function\"\"\"\n        super().__init__()\n\n        # layers 1,2,3 and non linear activation function (ReLU)\n        self.input = nn.Linear(in_features=2, out_features=10)\n        self.hidden1 = nn.Linear(in_features=10, out_features=10)\n        self.output = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Forward pass\n\n        Neural network with activation between layers\n\n        Args:\n            x: Input\n        \"\"\"\n        # neural network\n        forward_pass = self.output(self.relu(self.hidden(self.relu(self.input(x)))))\n        return forward_pass\n</code></pre> <p>The forward propagation through the neural network is constructed in the <code>forward()</code> method as seen in the snippet, where the outputs between each layer goes through the ReLU activation function. A visual representation of this neural network can be seen in the figure below</p> <p> </p>"},{"location":"Exercises/02/#exercise-3","title":"Exercise 3","text":"<p>As for the model developed in Chapter 1 we have to define a loss and optimizer function. For binary classification problems like this will the nn.BCEWithLogitsLoss()  loss function. This measures the Binary Cross Entropy loss between the target and input probabilities. This loss function outputs the logits from the neural network. Logits is the raw outputs from the last layer, and have to be transformed  to our labels in order for us to visualize how well our model is predicting. The optimizer function  (which in many cases is standard) used here is again the <code>nn.optim.SGD()</code> which implements the stochastic gradient descent</p>"},{"location":"Exercises/02/#exercise-4","title":"Exercise 4","text":"<p>Create a training and testing loop to fit the model you created in 2 to the data you created in 1.</p> <p>As for the model trained in Chapter 1 we follow the standard pattern with some extra calculations as shown in the code snippet below:</p> <ol> <li>Putting the model in training mode</li> <li>Running training data through the neural net and calculating predictions from logits</li> <li>Optimizer zero gradient</li> <li>Back propagate</li> <li>Step optimizer</li> <li>Evaluate</li> </ol> <pre><code>for epoch in range(epochs):\n    # set model in training mode\n    self.model_relu.train()\n\n    # forward pass\n    logits = self.model_relu(self.X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(logits))\n\n    # calculate loss\n    loss = self.loss_fn(logits, self.y_train)\n\n    # zero grad\n    self.optimizer_relu.zero_grad()\n\n    # back propagate\n    loss.backward()\n\n    # optimizer step\n    self.optimizer_relu.step()\n</code></pre> <p>I chose to evaluate the model with both training and testing data for each epoch, this because we wanted to finish the training of the model when it reached atleast 99% accuracy. To do this I used TorchMetrics  and the method <code>torchmetrics.functional.accuracy()</code>. To illustrate how the loss and accuracy evolves through  the training of this model I graphed out the loss and accuracy curves as seen in the figure below</p> <p> </p>"},{"location":"Exercises/02/#exercise-5","title":"Exercise 5","text":"<p>Make predictions with your trained model and plot them using the plot_decision_boundary() function created in this notebook.</p> <p>To further visualize how well trained our model is, and how well it can make predictions can a figure that plots the  boundaries of the decision be done. For this is a resource from  GitHub used called <code>plot_decision_boundary()</code>, which I modified to suit my style of using axes to plot which can be seen in the  code snippet below.</p> <pre><code>def plot_decision_boundary(ax, model: torch.nn.Module, X: torch.Tensor, y: torch.Tensor):\n    \"\"\"Plots decision boundaries of model predicting on X in comparison to y.\n\n    Source - https://madewithml.com/courses/foundations/neural-networks/ (with modifications)\n    \"\"\"\n    # Put everything to CPU (works better with NumPy + Matplotlib)\n    model.to(\"cpu\")\n    X, y = X.to(\"cpu\"), y.to(\"cpu\")\n\n    # Setup prediction boundaries and grid\n    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 101), np.linspace(y_min, y_max, 101))\n\n    # Make features\n    X_to_pred_on = torch.from_numpy(np.column_stack((xx.ravel(), yy.ravel()))).float()\n\n    # Make predictions\n    model.eval()\n    with torch.inference_mode():\n        y_logits = model(X_to_pred_on)\n\n    # Test for multi-class or binary and adjust logits to prediction labels\n    if len(torch.unique(y)) &gt; 2:\n        y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1)  # mutli-class\n    else:\n        y_pred = torch.round(torch.sigmoid(y_logits))  # binary\n\n    # Reshape preds and plot\n    y_pred = y_pred.reshape(xx.shape).detach().numpy()\n    ax.contourf(xx, yy, y_pred, cmap=plt.cm.RdYlBu, alpha=0.7)\n    ax.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.RdYlBu)\n    ax.set_xlim(xx.min(), xx.max())\n    ax.set_ylim(yy.min(), yy.max())\n</code></pre> <p>To use this function I start by initializing a figure object and then calling  the modified <code>plot_decision_boudary()</code> function with the axis, model, and test data. Below is this plot for the model trained here.</p> <p> </p>"},{"location":"Exercises/02/#exercise-6","title":"Exercise 6","text":"<p>Replicate the Tanh (hyperbolic tangent) activation function in pure PyTorch.</p> <p>Another activation function that is used in classification problems is the Tanh function which exist in the PyTorch library. In this exercise I am creating it myself in pure  PyTorch. See the code snippet below for the code and the figure below the snippet for an  comparison between PyTorchs own, and the one I developed.</p> <pre><code>def tanh(x: torch.Tensor) -&gt; torch.Tensor:\n    y = (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n    return y\n</code></pre> <p> </p>"},{"location":"Models/models/","title":"Models","text":""},{"location":"Models/models/#src.pytorch.models","title":"<code>src.pytorch.models</code>","text":""},{"location":"Models/models/#src.pytorch.models.LinearRegressionModel","title":"<code>LinearRegressionModel</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/pytorch/models.py</code> <pre><code>class LinearRegressionModel(nn.Module):\n    \"\"\"\n    Linear regression model for Chapter 1\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Init constructor\"\"\"\n\n        # call __init__ from nn.Module\n        super().__init__()\n\n        # set random seed\n        torch.random.manual_seed(42)\n\n        # initialize parameters with random values and require gradient\n        self.bias = nn.Parameter(torch.randn(\n            1, requires_grad=True, dtype=torch.float))\n        self.weight = nn.Parameter(\n            torch.randn(1, requires_grad=True, dtype=torch.float)\n        )\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"forward model\n\n        Method that computes runs data through\n        the neural net\n\n        Args:\n            x: Input data\n\n        Returns:\n            preds: prediction of the neural net\n        \"\"\"\n\n        # computation of the neural net\n        preds = self.weight * x + self.bias\n        return preds\n</code></pre>"},{"location":"Models/models/#src.pytorch.models.LinearRegressionModel.__init__","title":"<code>__init__()</code>","text":"Source code in <code>src/pytorch/models.py</code> <pre><code>def __init__(self):\n    \"\"\"Init constructor\"\"\"\n\n    # call __init__ from nn.Module\n    super().__init__()\n\n    # set random seed\n    torch.random.manual_seed(42)\n\n    # initialize parameters with random values and require gradient\n    self.bias = nn.Parameter(torch.randn(\n        1, requires_grad=True, dtype=torch.float))\n    self.weight = nn.Parameter(\n        torch.randn(1, requires_grad=True, dtype=torch.float)\n    )\n</code></pre>"},{"location":"Models/models/#src.pytorch.models.LinearRegressionModel.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input data</p> required <p>Returns:</p> Name Type Description <code>preds</code> <code>Tensor</code> <p>prediction of the neural net</p> Source code in <code>src/pytorch/models.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"forward model\n\n    Method that computes runs data through\n    the neural net\n\n    Args:\n        x: Input data\n\n    Returns:\n        preds: prediction of the neural net\n    \"\"\"\n\n    # computation of the neural net\n    preds = self.weight * x + self.bias\n    return preds\n</code></pre>"},{"location":"Models/models/#src.pytorch.models.MoonsModel_relu","title":"<code>MoonsModel_relu</code>","text":"<p>               Bases: <code>Module</code></p> Source code in <code>src/pytorch/models.py</code> <pre><code>class MoonsModel_relu(nn.Module):\n    \"\"\"\n    Classifier model with ReLU activation\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize layers and activation function\"\"\"\n        super().__init__()\n\n        # layers 1,2,3 and non linear activation function (ReLU)\n        self.input = nn.Linear(in_features=2, out_features=10)\n        self.hidden1 = nn.Linear(in_features=10, out_features=10)\n        self.output = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU()\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"Forward pass\n\n        Neural network with activation between layers\n\n        Args:\n            x: Input\n        \"\"\"\n        # neural network\n        forward_pass = self.output(self.relu(self.hidden1(self.relu(self.input(x)))))\n        return forward_pass\n</code></pre>"},{"location":"Models/models/#src.pytorch.models.MoonsModel_relu.__init__","title":"<code>__init__()</code>","text":"Source code in <code>src/pytorch/models.py</code> <pre><code>def __init__(self):\n    \"\"\"Initialize layers and activation function\"\"\"\n    super().__init__()\n\n    # layers 1,2,3 and non linear activation function (ReLU)\n    self.input = nn.Linear(in_features=2, out_features=10)\n    self.hidden1 = nn.Linear(in_features=10, out_features=10)\n    self.output = nn.Linear(in_features=10, out_features=1)\n    self.relu = nn.ReLU()\n</code></pre>"},{"location":"Models/models/#src.pytorch.models.MoonsModel_relu.forward","title":"<code>forward(x)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>x</code> <code>Tensor</code> <p>Input</p> required Source code in <code>src/pytorch/models.py</code> <pre><code>def forward(self, x: torch.Tensor):\n    \"\"\"Forward pass\n\n    Neural network with activation between layers\n\n    Args:\n        x: Input\n    \"\"\"\n    # neural network\n    forward_pass = self.output(self.relu(self.hidden1(self.relu(self.input(x)))))\n    return forward_pass\n</code></pre>"}]}