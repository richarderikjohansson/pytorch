{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Documentation for PyTorch course","text":"<p>Here will all documentation be stored for the YouTube course  relating to PyTorch. The course is  hosted for free on YouTube and can be found here. </p> <p>The course have 10 chapters in total, with exercises tied to each  chapter. GitHub repository, with the exercises can be found here and documentation for the course can be found here </p>"},{"location":"#data","title":"Data","text":"<p>One of the most fundamental things in Machine Learning is to split the data into:</p> <ol> <li>Training data</li> <li>Validation data</li> <li>Test data</li> </ol> <p>Where the training data is used to train the model to find the patterns you want. You then validate your model with the validation data to see how well it performs, and adjust it  if needed. Finally you use the test data to verify that the model is behaving as expected.</p>"},{"location":"Chapters/00/","title":"PyTorch Fundamentals","text":"<p>This chapter was entirely about working with tensors in  pytorch, as well working with random numbers. See  documentation on <code>torch.Tensor()</code> here, for creating <code>torch.rand()</code> here and seeding with <code>torch.random</code> here </p> <p>It is mostly about learning to navigate the documentation, and to get the habit of  turning to this when needed.</p>"},{"location":"Chapters/01/","title":"PyTorch Workflow Fundamentals","text":"<p>This chapter will cover topics such as:</p> <ul> <li>Data (preparing and loading)</li> <li>Building a simple model</li> <li>Fitting the model to data. i.e. training the model</li> <li>Making predictions and evaluating a model (inference)</li> <li>Saving and loading a model</li> </ul> <p> </p>"},{"location":"Chapters/01/#data","title":"Data","text":"<p>A very fundamental caveat around data in Machine Learning is  to split the data into mainly three different groups:</p> <ol> <li>Training data</li> <li>Validation data</li> <li>Test data</li> </ol> <p>The Training data is used to train the model to find the sought  patterns and parameters. The Validation data is used to validate  the model, and to make adjustments to it if needed. Finally  the Test data is used to see if the model is ready to for use.</p> <p>The typical splits is between 60-80% of your total data should be  used for training. 10-20% for validation and 10-20% for testing.  However, validation sets are not always used since the test set  also works as a validation set.</p>"},{"location":"Chapters/01/#building-models","title":"Building Models","text":"<p>What type of model you are building depend entirely on what the purpose of the model is. However, in pytorch you always build a class that  inherits from <code>torch.nn.Module</code>. This class contains all the necessary  building blocks needed to build any Machine Learning model. See nn.Module for the documentation.</p> <p>Below will is an example of a very simple model that will inherit from  <code>nn.Module</code> and is a linear regression model.</p> <pre><code>class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return \n</code></pre> <p>In line 3 in the snippet above is the initialization class called from  <code>nn.Module</code> which contains all necessary methods to compute. One important  thing method seen in the snippet above is the <code>forward()</code>  method, which  overwrites <code>nn.Module</code>s one. This one is where we define what the model is supposed to do. In this case we have not yet defined this.</p> <p>As a summary we have a few essential modules we will be using when building such a simple model as the one above.</p> <ul> <li><code>torch.nn</code> contains all of the building blocks for a neural network</li> <li><code>torch.nn.Parameter</code> contains what parameters that the model should try to learn. often a PyTorch layer which will be set for us.</li> <li><code>torch.nn.Module</code> is the base class for all neural networks, and if you inherit from it, you should overwrite the <code>forward()</code> method.</li> <li><code>torch.optim</code> contain various optimization algorithms.</li> </ul> <p>A good place to look in order to see what various PyTorch modules do is to  look into the PyTorch cheat sheet </p>"},{"location":"Chapters/01/#model-training","title":"Model Training","text":"<p>In order to make the model to predict data it has to be trained. It is done  with the training data set. Down below we see essential blocks needed for training the model</p>"},{"location":"Chapters/01/#loss-functions-and-optimizers","title":"Loss functions and Optimizers","text":"<ul> <li>Loss Function A function to measure how wrong the models predictions are</li> <li>Optimizer Uses the loss of the model and adjust the models parameters to make the predictions better, by minimizing the loss function.</li> </ul> <p>To see which built in loss functions ans optimizers that exist within PyTorch see Loss Functions  and  Optimizers. What is  essential in all models is that these have to be picked to the developer, and which is the right one to use depend on the model that is being built.</p> <p>Another important parameter that the developer have to choose is the hyperparameter <code>lr</code> which stand for learning rate. This tells the optimizer how much the parameter values should change in order to decrease the loss. </p>"},{"location":"Chapters/01/#training-loop","title":"Training loop","text":"<p>The training loop is where the model is being trained in order to update the parameter values. There is a few essential steps in this loop that need to happen, and these are listed below:</p> <p>Loop through the data and: 1. forward pass (moving the input data through the network) to give a output on the current parameters  2. calculate the loss with the loss function  3. optimizer zero grad, resets the gradients of the optimized tensor. 4. back propagation moves the data through the network backwards (calculate the loss gradients) 5. optimizer step, refining the parameters (gradient descent)</p> <p>This is the most common order to do this in </p>"},{"location":"Exercises/00/","title":"PyTorch Fundamentals","text":""},{"location":"Exercises/00/#exercise-1","title":"Exercise 1","text":"<p>A big part of deep learning (and learning to code in general) is getting familiar with the documentation of a certain framework you're using. We'll be using the PyTorch documentation a lot throughout the rest of this course. So I'd recommend spending 10-minutes reading the following (it's okay if you don't get  some things for now, the focus is not yet full understanding, it's awareness):</p> <ul> <li>The tensor datatype has a lot of methods and attributes available, will go  back when needing more information </li> <li>Utilizing CUDA threads is a very good way to speed up the software, instead of running all operations on the CPU. On my current machine (MacBook Air M2)  I do not have CUDA cores available, but will dive into this when I have my  desktop PC running again from my office.</li> </ul>"},{"location":"Exercises/00/#exercise-2","title":"Exercise 2","text":"<p>Create a random tensor with shape <code>(7, 7)</code>.</p> <p>Create this random tensor with <code>torch.rand()</code>:</p> <pre><code>def e2():\n    random_tensor = torch.rand(7, 7)\n    print(\"Shape: \", random_tensor.shape)\n</code></pre>"},{"location":"Exercises/00/#exercise-3","title":"Exercise 3","text":"<p>Perform matrix multiplication on the tensor from Exercise 2 with another random tensor with shape <code>(1, 7)</code></p> <p>To perform matrix multiplication we do this with <code>torch.matmul</code>. But we have to transpose the second tensor since the number of rows do not align with the number or columns in the first tensor. This is done with the  <code>transpose()</code> before making the matrix multiplication:</p> <pre><code>def e3():\n    rt1 = torch.rand(7, 7)\n    rt2 = torch.rand(1, 7)\n    rt2_t = rt2.transpose(0, 1)\n    mat_multi = torch.matmul(rt1, rt2_t)\n\n    print(f\"Matrix multiplication {mat_multi=}\",\n          f\"With shape {mat_multi.shape=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-4","title":"Exercise 4","text":"<p>Set the random seed to <code>0</code> and do exercises 2 and 3 again</p> <p>We are setting the random seed on the CPU with <code>torch.random.manual_seed()</code>. Then we perform the same as in  exercises 2 and 3: </p> <pre><code>def e4():\n    torch.random.manual_seed(0)\n    rt1 = torch.rand(size=(7, 7))\n    rt2 = torch.rand(size=(1, 7))\n    rt2_t = rt2.transpose(0, 1)\n    mat_multi = torch.matmul(rt1, rt2_t)\n    print(f\"Matrix multiplication {mat_multi=}\",\n          f\"With shape {mat_multi.shape=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-5","title":"Exercise 5","text":"<p>Speaking of random seeds, we saw how to set it with <code>torch.manual_seed()</code> but is there a GPU equivalent?  (hint: you'll need to look into the documentation for <code>torch.cuda</code> for this one)</p> <p>Since I do not have CUDA cores available we skip this exercise for now. But you set the random seed  on one CUDA core with <code>torch.cuda.manual_seed()</code></p> <pre><code>def e5():\n    print(f\"CUDA: {torch.cuda.is_available()=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-6","title":"Exercise 6","text":"<p>Create two random tensors of shape <code>(2, 3)</code> and send them both to the GPU (you'll need access to a GPU for this).  Set <code>torch.manual_seed(1234)</code> when creating the tensors (this doesn't have to be the GPU random seed).</p> <p>As in exercise 5, we do not have CUDA cores available. But we can check this with <code>torch.cuda.is_available()</code>. So  I do this and sets the seed on the CUDA cores if they are available, if not I seed the CPU and do what is asked and  returning the tensors, since they are used in the next exercise:</p> <pre><code>def e6():\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(1234)\n    else:\n        torch.random.manual_seed(1234)\n\n    rt1 = torch.rand(2, 3)\n    rt2 = torch.rand(2, 3)\n    print(f\"{rt1=}\")\n    print(f\"{rt2=}\")\n\n    return rt1, rt2\n</code></pre>"},{"location":"Exercises/00/#exercise-7","title":"Exercise 7","text":"<p>Perform a matrix multiplication on the tensors you created in exercise 6 (again, you may have to adjust the shapes of one of the tensors).</p> <p>Since the number of rows in the second tensor not is matching the number of columns in the first tensor I have to transpose again before doing the matrix multiplication with <code>torch.matmul</code>. I also return the  matrix product since I have to use this in exercises 8 and 9:</p> <pre><code>def e7(rt1: torch.Tensor, rt2: torch.Tensor):\n    rt2_t = rt2.transpose(0, 1)\n    mat_multi = torch.matmul(rt1, rt2_t)\n    return mat_multi\n    print(f\"{mat_multi=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-8","title":"Exercise 8","text":"<p>Find the maximum and minimum values of the output of exercise 7</p> <p>This is done with the method <code>max()</code> and <code>min()</code> which is attributes to the tensor class:</p> <pre><code>def e8(mat_multi: torch.Tensor):\n    mx = mat_multi.max()\n    mn = mat_multi.min()\n    print(f\"{mat_multi=}\\n max: {mx=} \\n min: {mn=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-9","title":"Exercise 9","text":"<p>Find the maximum and minimum index values of the output of exercise 7.</p> <p>This is done with the <code>argmin()</code> and <code>argmax()</code> methods:</p> <pre><code>def e9(mat_multi: torch.Tensor):\n    argmx = mat_multi.argmax()\n    argmn = mat_multi.argmin()\n    print(f\"{mat_multi=}\\n argmax: {argmx=} \\n argmin: {argmn=}\")\n</code></pre>"},{"location":"Exercises/00/#exercise-10","title":"Exercise 10","text":"<p>Make a random tensor with shape <code>(1, 1, 1, 10)</code> and then create a new tensor with all the 1 dimensions removed to be left with a tensor of shape <code>(10)</code>.  Set the seed to 7 when you create it and print out the first tensor and it's shape as well as the second tensor and it's shape.</p> <p>Once again I set the seed with <code>torch.random.manual_seed()</code> and then I am defining the first multi dimensional tensor with the  <code>size</code> argument. I then create a new tensor by selecting the first indices in the first tensor</p> <pre><code>def e10():\n    torch.random.manual_seed(7)\n    rt1 = torch.rand(size=(1, 1, 1, 10))\n    rt2 = rt1[0, 0, 0]\n\n    print(rt1, rt1.shape)\n    print(rt2, rt2.shape)\n</code></pre>"},{"location":"Exercises/01/","title":"PyTorch Workflow Fundamentals","text":"<p>In this chapter some of the most fundamental parts in  building an Machine Learning model is being gone through. All the way from splitting the data into Training and  Testing sets to saving the trained model.</p> <p>The workflow that this model entails is one with a ground thruth, meaning that we actually create the data from a linear regression  formula. But even though we actually know the parameters that our  model will find in the data will this still be a good first model to build.</p>"},{"location":"Exercises/01/#exercise-1","title":"Exercise 1","text":"<p>Create a straight line dataset using the linear regression formula <code>(weight * X + bias)</code>.  Set weight to 0.3 and bias to 0.9 and split the data into 80% training and 20% testing and  plot the data </p> <pre><code>def e1(self):\n    \"\"\"First exercise of Chapter 1\n\n    In this exercise will data be created using the linear\n    regression formula. The data will be split in training and\n    testing data (80/20 split) and plot the training and testing\n    data for visualization\n    \"\"\"\n    self.logger.info(\"--- Running Exercise 1 ---\")\n    bias = 0.9\n    weight = 0.3\n    start = 0\n    end = 1\n    step = 0.005\n\n    X = torch.arange(start, end, step).unsqueeze(dim=1)\n    y = weight * X + bias\n\n    split = int(0.8 * len(X))\n\n    self.X_train = X[:split]\n    self.y_train = y[:split]\n    self.X_test = X[split:]\n    self.y_test = y[split:]\n\n    self.logger.info(\"Plotting training and testing data\\n\")\n    plot_predictions(self.X_train,\n                     self.y_train,\n                     self.X_test,\n                     self.y_test,\n                     figname=self.assetsdir / \"ch1e1.png\")\n</code></pre> <p>I created the data by utilizing the <code>torch.arange()</code> function and then using the linear regression  formula to create the outputs seen in lines 16 and 17. I then split the data in by finding index of  80% of the data and then using slicing to order the data into training, and testing sets. This can be seen in lines 19-24. I then created a plotting function to plot this data, which also could be used  to plot predictions which is when the model is being used on testing data. Seen in the figure below  is only the training and testing data</p> <p> </p> <pre><code>def plot_predictions(train_data,\n                     train_labels,\n                     test_data,\n                     test_labels,\n                     figname,\n                     predictions=None):\n\n    gs = GridSpec(1, 1)\n    fig = plt.figure(figsize=(10, 7))\n    ax = fig.add_subplot(gs[0, 0])\n    ax.set_title(\"Traning and testing data\")\n    ax.scatter(train_data, train_labels, c=\"dimgray\", s=4, label=\"Training Data\")\n    ax.scatter(test_data, test_labels, c=\"orange\",  s=4, label=\"Test Data\")\n\n    if predictions is not None:\n        ax.scatter(test_data,\n                   predictions,\n                   c=\"tomato\",\n                   s=4,\n                   label=\"Predictions\")\n\n    ax.legend()\n    fig.savefig(figname, transparent=False)\n    plt.close()\n</code></pre>"},{"location":"Exercises/01/#exercise-2","title":"Exercise 2","text":"<p>Build a PyTorch model by subclassing <code>nn.Module</code>. In the model should parameters  be initialized using the <code>nn.Parameters</code> used, setting it to random values. One parameter for the weight for the bias. a <code>forward()</code> model should also be created to compute the linear  regression. Also make an instance of the model and print the current parameters on the untrained model.</p> <p>The way of doing this is by creating a new class that inherits from the <code>nn.Module</code> class. The <code>nn.Module</code> class contains more or less every building block necessary for any kind of Neural Net. Below is my implementation:</p> <pre><code>class LinearRegressionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.random.manual_seed(42)\n        self.bias = nn.Parameter(torch.randn(1, \n                                             requires_grad=True, \n                                             dtype=torch.float))\n        self.weight = nn.Parameter(torch.randn(1, \n                                               requires_grad=True, \n                                               dtype=torch.float))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.weight * x + self.bias\n</code></pre> <p>As instructed I created parameters for the bias and the weight using the <code>nn.Parameters</code> class, and setting those parameters to random values with <code>torch.randn</code>. Seen in the creation of these, I set the argument <code>requires_grad</code> to True. This is needed if gradient descent is used  to find these values in the training, which is the most standard way of solving a problem of this  kind. </p> <p>I also implemented a <code>forward()</code> method for this model, which always has to be done when inheriting from <code>nn.Module</code>. This method is used to propagate through the neural net and will do the predictions from the input data.</p> <p>To initialize this model I just call this model, and to print out the current parameters is the method <code>state_dict()</code> called on the newly initialized model object, as seen below</p> <pre><code>def e2(self):\n    \"\"\"Second exercise of Chapter 1\n\n    In this we shall construct a linear regression model with\n    randomized parameters. We should also construct a forward\n    method that gives the prediction from the input and\n    parameters. We should the initiate this model and print out\n    the current parameters.\n    \"\"\"\n    self.logger.info(\"--- Running Exercise 2 ---\")\n    self.logger.info(\"Initiating the model\")\n    self.model = LinearRegressionModel()\n    params = self.model.state_dict()\n    self.logger.info(f\"Current parameters:\\n {params}\\n\")\n</code></pre>"},{"location":"Exercises/01/#exercise-3","title":"Exercise 3","text":"<p>Create a loss function and optimizer using nn.L1Loss() and torch.optim.SGD(params, lr) respectively. Set the learning rate of the optimizer to be 0.01 and the parameters to optimize should be the model parameters from the model you created in 2. Write a training loop to perform the appropriate training steps for 300 epochs. The training loop should test the model on the test dataset every 20 epochs.</p> <p>The loss function and the optimizer is two of the most fundamental things there is when it comes  to machine learning. </p> <ul> <li> <p>The loss function dictates how the loss is measured, meaning how much the predicted data deviates from the expected output. This function will depend on what type of problem you are trying to solve. But in this exercise I was instructed to use the <code>nn.L1Loss()</code> function, which actually measures the mean absolute error.</p> </li> <li> <p>The optimizer is a function that tells the model how the parameters should be updated. These functions also take the very important hyperparameter <code>lr</code>, which is the learning rate. This  hyperparameter is very important because it tells the model how aggressively the model should update  the models parameters. The optimizer we were instructed to use here implements stochastic gradient descent and is found in the <code>torch.optim</code> module.</p> </li> </ul> <p>The training loop typically follows the below pattern, and in the code snippet below is my implementation</p> <ol> <li>forward pass (moving the input data through the network) to give a output on the current parameters </li> <li>calculate the loss with the loss function </li> <li>optimizer zero grad, resets the gradients of the optimized tensor.</li> <li>back propagation moves the data through the network backwards (calculate the loss gradients)</li> <li>optimizer step, refining the parameters (gradient descent)</li> </ol> <pre><code>    def e3(self, epochs):\n        \"\"\"Third exercise of Chapter 1\n\n        In this exercise we are supposed to develop the training\n        loop and also print out how the loss progresses\n\n        Args:\n            epochs (int): Number of times the model should fun through the NN\n        \"\"\"\n        self.logger.info(\"--- Running Exercise 3 ---\")\n        self.logger.info(\"Creating loss and optimizer functions\")\n        self.loss = nn.L1Loss()\n        self.optimizer = torch.optim.SGD(\n            params=self.model.parameters(),\n            lr=0.01\n        )\n\n        self.epochs = epochs\n        self.logger.info(\"Entering training loop\")\n\n        # training loop\n        for epoch in range(self.epochs):\n\n            # set model into training mode\n            self.model.train()\n\n            # forward pass through network\n            y_pred = self.model(self.X_train)\n\n            # calculate loss\n            loss = self.loss(y_pred, self.y_train)\n\n            # set optimizer to zero gradient\n            self.optimizer.zero_grad()\n\n            # back propagate\n            loss.backward()\n\n            # step the optimizer\n            self.optimizer.step()\n\n            # enter testing mode\n            self.model.eval()\n            with torch.inference_mode():\n                test_pred = self.model(self.X_test)\n                test_loss = self.loss(test_pred, self.y_test)\n\n                if epoch % 20 == 0:\n                    print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n\n        self.logger.info(\"Training done\\n\")\n</code></pre>"},{"location":"Exercises/01/#exercise-4","title":"Exercise 4","text":"<p>Make predictions with the trained model on the test data. Visualize these predictions against the original training and testing data  (note: you may need to make sure the predictions are not on the GPU if you want to use non-CUDA-enabled libraries such as matplotlib to plot).</p> <p>This exercise is all about visualizing how well our model predicts the testing data. To get the predictions I first put the model in evaluate mode with the method <code>eval()</code> and do the prediction in inference mode with  <code>torch.inference_mode()</code>. This can be seen in the code snippet below, and the visualization of the training, testing  and predicted data can be seen in the figure below</p> <p><pre><code>def e4(self):\n    self.logger.info(\"--- Running Exercise 4 ---\")\n    self.model.eval()\n    with torch.inference_mode():\n        predictions = self.model(self.X_test)\n\n    self.logger.info(\"Plotting the predictions\\n\")\n    plot_predictions(\n        train_data=self.X_train,\n        train_labels=self.y_train,\n        test_data=self.X_test,\n        test_labels=self.y_test,\n        figname=self.assetsdir / \"ch1e4.png\",\n        predictions=predictions\n    )\n</code></pre> </p>"},{"location":"Exercises/01/#exercise-5","title":"Exercise 5","text":"<p>Save your trained model's state_dict() to file. Create a new instance of your model class you made in 2. and load in the state_dict() you just saved to it. Perform predictions on your test data with the loaded model and confirm they match the original model predictions from 4.</p> <p>To save the trained model I created a separate function as can be seen below <pre><code>def save_model(model, name):\n    modeldir = Path(\"models\")\n    basedir = Path(__file__)\n    savedir = basedir.parent / modeldir\n\n    if not savedir.exists():\n        savedir.mkdir()\n\n    torch.save(obj=model.state_dict(),\n               f=savedir / name)\n</code></pre></p> <p>Here I pass in the model as one of the argument and a second argument, name which will be under what name the models parameters will be saved. I also create a directory called <code>models/</code> where the model will be saved.</p> <p>TODO * create function for loading the model and also ensure that the loaded model will give the same  predicted values as the model that was saved.</p>"}]}